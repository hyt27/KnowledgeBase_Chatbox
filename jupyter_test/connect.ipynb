{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "110f2910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# 定义 Embeddings\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"/home/lrctadmin/Documents/LLM/self-llm/models/Qwen/Langchain/embedding_model/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"/home/lrctadmin/Documents/LLM/self-llm/models/Qwen/Langchain/embedding_model/bge-m3\")\n",
    "\n",
    "# 向量数据库持久化路径\n",
    "persist_directory = '/home/lrctadmin/Documents/LLM/self-llm/models/Qwen/Langchain/data_base/vector_db/bge-m3'\n",
    "\n",
    "# 加载数据库\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory, \n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88931611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.chroma.Chroma object at 0x7c0a2fe988b0>\n"
     ]
    }
   ],
   "source": [
    "print(vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e4bac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [00:04<00:00,  1.62it/s]\n",
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Loading the model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我是通义千问，由阿里云开发的语言模型。我被设计用来回答各种问题、提供信息和与用户进行对话。有什么我可以帮助你的吗？'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LLM import QwenLM\n",
    "#llm = QwenLM(model_path = \"/root/autodl-tmp/qwen\")\n",
    "llm = QwenLM(model_path = \"/home/lrctadmin/Documents/LLM/self-llm/models/Qwen/Langchain/LLM_model/Qwen-7B-Chat\")\n",
    "llm.predict(\"你是谁\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e380e9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████| 10/10 [00:05<00:00,  1.69it/s]\n",
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Loading the model\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。\n"
     ]
    }
   ],
   "source": [
    "from LLM import ChatGLM4_LLM\n",
    "gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "llm = ChatGLM4_LLM(mode_name_or_path=\"/home/lrctadmin/Documents/LLM/self-llm/models/GLM-4/glm-4-9b-chat\", gen_kwargs=gen_kwargs)\n",
    "print(llm.invoke(\"你是谁\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f83c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 我们所构造的 Prompt 模板\n",
    "template = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。尽量使答案简明扼要。总是在回答的最后说“谢谢你的提问！”。\n",
    "{context}\n",
    "问题: {question}\n",
    "有用的回答:\"\"\"\n",
    "\n",
    "# 调用 LangChain 的方法来实例化一个 Template 对象，该对象包含了 context 和 question 两个变量，在实际调用时，这两个变量会被检索到的文档片段和用户提问填充\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2942e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectordb.as_retriever(),return_source_documents=True,chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1fed27",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181280b",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c7a4a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索问答链回答 question 的结果：\n",
      "\n",
      "QwenLM 是一个大型语言模型，由 Qwen 团队开发。它在排行榜上表现突出，具有 72B 的参数规模，并支持指令微调。QwenLM 在自然语言处理任务中表现出色，包括问答、文本生成等。\n",
      "\n",
      "谢谢你的提问！\n"
     ]
    }
   ],
   "source": [
    "question = \"什么是QwenLM\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(\"检索问答链回答 question 的结果：\")\n",
    "print(result[\"result\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d403017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大模型回答 question 的结果：\n",
      "\n",
      "QwenLM 是由清华大学 KEG 实验室和智谱AI共同开发的一款大型语言模型。它基于清华大学 KEG 实验室提出的 GLM-4 模型，并针对中文进行了优化。QwenLM 具有强大的语言理解和生成能力，可以应用于文本生成、机器翻译、问答系统、文本摘要等多种自然语言处理任务。\n",
      "\n",
      "QwenLM 的特点包括：\n",
      "\n",
      "1. 强大的语言理解能力：QwenLM 能够理解复杂的中文文本，并从中提取关键信息。\n",
      "\n",
      "2. 高效的文本生成能力：QwenLM 可以根据输入的文本生成高质量的文本，如新闻报道、故事、诗歌等。\n",
      "\n",
      "3. 优秀的机器翻译能力：QwenLM 在机器翻译任务上表现出 }];\n",
      "4. 丰富的应用场景：QwenLM 可以应用于多种自然语言处理任务，如文本生成、问答系统、文本摘要等。\n",
      "\n",
      "QwenLM 的出现为中文自然语言处理领域带来了新的突破，有助于推动相关技术的发展和应用。\n"
     ]
    }
   ],
   "source": [
    "# 仅 LLM 回答效果\n",
    "result_2 = llm(question)\n",
    "print(\"大模型回答 question 的结果：\")\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf86bfc",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dac2ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The typical horizontal spacing between steel struts used in bottom-up construction in Hong Kong is between 3 m and 9 m.\n",
      "\n",
      "谢谢你的提问！\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the typical horizontal spacing between steel struts used in bottom-up construction in Hong Kong?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2acf2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The horizontal spacing between steel struts in bottom-up construction in Hong Kong can vary depending on the specific design requirements, the type of structure, and the engineering standards being followed. However, as a general guideline, the following are common spacings used:\n",
      "\n",
      "1. For residential buildings: The horizontal spacing between steel struts is often around 1.5 meters (5 feet) to 2 meters (6.5 feet) apart.\n",
      "\n",
      "2. For commercial buildings: The spacing might be slightly larger, typically around 2 meters (6.5 feet) to 3 meters (10 feet) apart.\n",
      "\n",
      "3. For industrial buildings: The spacing can be wider, often around 3 meters (10 feet) to 4 meters (13 feet) apart.\n",
      "\n",
      "It's important to note that these are general guidelines and actual spacings may differ based on the specific project requirements, structural analysis, and local building codes. It is always best to consult with a structural engineer or architect for the precise spacing required for a particular project.\n"
     ]
    }
   ],
   "source": [
    "# 仅 LLM 回答效果\n",
    "result_2 = llm(question)\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b959f80",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb4d73b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The new leaderboard ranking includes the following models:\n",
      "\n",
      "1. Qwen/Qwen2-72B-Instruct\n",
      "2. oMANOUR WD microsoft/Phi-3-medium-4k-instruct\n",
      "3. 01-ai/Yiorical-1.5-34B-Chat\n",
      "4. abacusai/Smaug-72B-v0.1\n",
      "5. Qwen/Qwen1.5-110B-Chat\n",
      "6. microsoft/Phi-3-small-128k-instruct\n",
      "7. meta-llama/Meta-Llama-3-70B-Instruct\n",
      "8. CohereForAl/c4ai-command-r-plus\n",
      "9. 01-ai/Yi-1.5-9B-Chat\n",
      "\n",
      "Thanks for your question!\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the new leaderboeard ranking\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa74776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/lrctadmin/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'm sorry, but I don't have real-time access to external databases or the internet to provide current leaderboards or rankings for any specific game, competition, or event. If you're referring to a specific leaderboard, such as in a video game\\Twig, sports competition, or any other context, you would need to check the official source or platform where the leaderboard is maintained.\n",
      "\n",
      "If you can provide more details about the specific leaderboard you're interested in, I can give you general advice on how to find the latest rankings.\n"
     ]
    }
   ],
   "source": [
    "# 仅 LLM 回答效果\n",
    "result_2 = llm(question)\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a5dee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
